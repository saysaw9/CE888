# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/saysaw9/CE888/blob/master/CE888%20Reassessment.ipynb
"""

#Important Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from google.colab import files
import io

data = files.upload()

#Dataset
df = pd.read_csv(io.StringIO(data['csv_result-bone-marrow.csv'].decode('utf-8')))
df.head()

df.dtypes

df.shape

df.columns

df.describe()

df.drop(['id','recipient_age_below_10','recipient_age_int','gender_match','HLA_mismatch','antigen','acute_GvHD_II_III_IV','acute_GvHD_III_IV'],axis=1, inplace=True)

df

"""after removing the unnecessarry columns, it is now important to check the two main columns, the ABO match and the recipient ABO

"""

df.nunique()

df['ABO_match'].unique()

df['recipient_ABO'].unique()

"""we see that there are categorical as well as numerical values in the columns. Seperating them can be useful.


"""

def extract_cat_num(df):
    cat_col=[col for col in df.columns if df[col].dtype=='object']
    num_col=[col for col in df.columns if df[col].dtype!='object']
    return cat_col,num_col

cat_col,num_col=extract_cat_num(df)

cat_col

num_col

"""checking the columns again to see if all the values are categorical only in cat_col"""

for col in cat_col:
    print('{} has {} values '.format(col,df[col].unique()))
    print('\n')

"""here we see there are still columns with numerical data. Hence we need to convert and transfer to the numerical columns

"""

#coverting data_types
def convert_dtype(df,feature):
    df[feature] = pd.to_numeric(df[feature],errors='coerce')

features =['CD3_x1e8_per_kg','CD3_to_CD34_ratio','recipient_body_mass','survival_status']
for feature in features:
    convert_dtype(df,feature)

def extract_cat_num(df):
    cat_col=[col for col in df.columns if df[col].dtype=='object']
    num_col=[col for col in df.columns if df[col].dtype!='object']
    return cat_col,num_col
cat_col,num_col=extract_cat_num(df)

cat_col

for col in cat_col:
    print('{} has {} values '.format(col,df[col].unique()))
    print('\n')

"""As we can see all the values are converted are separated properly in categorical and numerical values.
Now lets check the null values
"""

df.isnull().sum()

df1 = df.dropna()
print(df1)

df1[num_col].isnull().sum()

df1.info()

"""Now that we have all the values, lets start by plotting some graphs. here we plot the graphs for the numerical data. Lets see first the gender of the recipient from the categorical columns."""

sns.countplot(df1['recipient_gender'])

"""We see more than 100 nos. of the recipients are male and females upto 70. through the next plots we can gain insights about age and survival time as well."""

##########Ploting Data##########
plt.figure(figsize=(30,20))

for i, feature in enumerate(num_col):
    plt.subplot(5,4,i+1)
    df1[feature].hist()
    plt.title(feature)

"""From this we understand and get details about columns like donor_age, recipient_age, survival_time.
Now lets find some correlation between the columns to understand in deep and see which columns are highly correlated to each other.
"""

df1.corr()

"""lets plot this correlation with a heatmap to understand better."""

plt.figure(figsize=(7,5))
df1.corr()
sns.heatmap(df1.corr(),annot=True)

"""As we see the column CD34_xle6_per_kg is highly correlated to recipient_body_mass. lets see some more plots and relations."""

sns.lmplot(x='survival_time',y='recipient_age', hue = 'recipient_gender',data=df1)

"""As we can see the survival age is mostly from the age between 10 to 11 years."""

sns.lmplot(x='donor_age', y='recipient_age', data=df1)

"""From this we can see the recipient age is young, not even 15 years compared to the donors who range from 20 to 45 years of age.

"""

sns.lmplot(x='PLT_recovery',y='recipient_age', hue = 'survival_status',data=df1)

"""We can see that there is a high chance of survival_status with increase in platlette counts."""

px.violin(df1,y= 'recipient_age',x ='disease_group')

"""The violin grapgh consists of the disease groups that are malignant and nonmalignant. Through this we understand the quartiles q1 and q3, the median, the kernel density and the minimum and the maximum age."""

px.scatter(df1,x='donor_age', y =  'recipient_age')

sns.swarmplot(x='ANC_recovery', y='PLT_recovery', data=df1, 
              hue='disease')

##### Showing info about disease and recipent age##########
grid = sns.FacetGrid(df1,hue='disease',aspect = 3)
grid.map(sns.kdeplot,'recipient_age')
grid.add_legend()

"""We can see that the highest is lymphoma which affects the children's age ranging from about 1 year to 25 years, followed by nonmalignant, AML, A11 and chronic diseases."""

grid = sns.FacetGrid(df1,hue='disease',aspect = 3)
grid.map(sns.kdeplot,'recipient_body_mass','survival_time')
grid.add_legend()

sns.kdeplot(df1.recipient_body_mass, df.survival_time)

fig = px.scatter_matrix(df1, dimensions=["donor_age", "recipient_age","CD3_to_CD34_ratio",'survival_time'], color="risk_group")
fig.show()

"""Based on this scatter plot, higher the donor age, higher the recipient age, higher are chances of risk. """

fig = px.scatter(df1, x="donor_age", y="recipient_age", color="disease", marginal_y="violin",
           marginal_x="box", trendline="ols", template="simple_white")
fig.show()

def violin(col):
    fig=px.violin(df1,y=col,x='survival_status',color = 'disease',box=True)
    return fig.show()

violin('recipient_body_mass')

def kde_plot(feature):   
    grid = sns.FacetGrid(df1,hue='survival_status',aspect = 2)
    grid.map(sns.kdeplot,feature)
    grid.add_legend()

kde_plot('recipient_age')

def scatters(col1,col2):
    fig = px.scatter(df1,x=col1,y=col2,color='survival_status')
    return fig.show()

scatters('recipient_age','survival_time')

"""From the kde plot and the scatter plot we can see that the survival time is independent of the recipients age, since even after recovery the survival time depends on every individual and here it shows it decreases in all cases."""

df1.info()

df1.isnull().sum().sort_values(ascending = True)

df1[num_col].isnull().sum()

df1[cat_col].isnull().sum()

"""Data is clean and does not contain any null values."""

df1.head()

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

le = LabelEncoder()
data = df1

for col in cat_col:
    data[col]=le.fit_transform(data[col])

data.head()

independent_column =[col for col in data.columns if col!='survival_status']
dependent_column='survival_status'

X=data[independent_column]
y=data[dependent_column]

X.head()

Y.head()

"""Applying SelectKBest to select the top 10 best features using the score function chi square"""

BestFeatures=SelectKBest(score_func=chi2, k=20)
Fit=BestFeatures.fit(X,y)

dfscores=pd.DataFrame(Fit.scores_)
dfcolumns=pd.DataFrame(X.columns)

Feature_Scores = pd.concat([dfcolumns,dfscores],axis=1)
Feature_Scores

"""the features with the hoghest scores are the most correlated features which gives us the most survival status."""

datascores = pd.DataFrame(Fit.scores_,columns=['Score'])

Feature_Scores = pd.concat([dfcolumns,datascores],axis=1)
Feature_Scores

Feature_Scores.nlargest(10,'Score')

from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X,y)

print(model.feature_importances_)

Feature_Importances = pd.Series(model.feature_importances_, index=X.columns)
Feature_Importances.nlargest(10).plot(kind = 'barh')
plt.show()

important_columns = Feature_Scores.nlargest(10,'Score')[0].values

X_new = data[important_columns]
X_new.head()

"""Split into Test and Train"""

from sklearn.model_selection import train_test_split

X_train,X_test, y_train,y_test =  train_test_split(X_new,y,test_size=0.25,random_state=109)

X_new.shape, y.shape

X_new.iloc[181]

"""Fit and Model Training"""

import sklearn.ensemble as skle

"""creating and fitting the model"""

rfc = skle.RandomForestClassifier()
rfc.fit(X_train, y_train)

from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import accuracy_score
accuracy=[]
skf = StratifiedKFold(n_splits=10, random_state=None)
skf.get_n_splits(X_new,y)

for train_index, test_index in skf.split(X_new,y):
  print("Train:", train_index, "Validation:", test_index)
  X1_train, X1_test = X_new.iloc[train_index], X_new.iloc[test_index]
  y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]
  
  rfc.fit(X1_train,y1_train)
  prediction=rfc.predict(X1_test)
  score = accuracy_score(prediction,y1_test)
  accuracy.append(score)

  print(accuracy)

np.array(accuracy).mean()

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

"""Feature Scaling"""

sc = StandardScaler()
X_train = sc.fit_transform(X1_train)
y_train = sc.fit_transform(X1_test)

print(f'X1_train :{X1_train.shape}')
print(f'y1_train :{y1_train.shape}')
print(f'X1_test :{X1_test.shape}')
print(f'y1_test :{y1_test.shape}')

n_estimators = [int(x) for x in np.linspace(start=10, stop=80, num=10)]
max_features = ['auto', 'sqrt']
max_depth = [2,4]
min_samples_split = [2,5]
min_samples_leaf = [1,2]
bootstrap = [True, False]

param_grid = {
    'n_estimators':n_estimators,
    'max_features':max_features,
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'bootstrap': bootstrap}
print(param_grid)

rfc_grid = GridSearchCV(estimator= rfc, param_grid= param_grid, cv = 3, verbose = 2, n_jobs=4)

rfc_grid.fit(X1_train,y1_train)

rfc_grid.best_params_

print(f'Train Accuracy - : {rfc_grid.score(X1_train,y1_train):.3f}')
print(f'Test Accuracy - : {rfc_grid.score(X1_test,y1_test):.3f}')

"""Fittinmg SVM to training set"""

from sklearn.svm import SVC

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 5)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

Classifier = SVC(kernel = 'linear', random_state = 0)
Classifier.fit(X_train,y_train)

Y_prediction = Classifier.predict(X_test)

"""Confusion Matrix

"""

from sklearn.metrics import confusion_matrix

CM = confusion_matrix(y_test, Y_prediction)

accuracy = accuracy_score(y_test, Y_prediction)

accuracy

# Applying Grid Search 
from sklearn.model_selection import GridSearchCV
parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search = GridSearchCV(estimator = Classifier,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search = grid_search.fit(X_train, y_train)

accuracy = grid_search.best_score_

accuracy

grid_search.best_params_

classifier = SVC(kernel = 'rbf', gamma=0.7)
classifier.fit(X_train, y_train)

# Prediction of the Test set results
y_prediction = classifier.predict(X_test)

# Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_prediction)\

accuracy=accuracy_score(y_test,y_prediction)

accuracy

"""This shows that SVM with rbf kernel is not suitable for the data set

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

logmodel = LogisticRegression()

parameters_grid = [
    {
     'penalty':['l1','l2','eleasticent','none'],
     'C': np.logspace(-4,4,20),
     'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],
     'max_iter': [100,1000,2500,5000]
    }
]

clf_lin = GridSearchCV(logmodel, param_grid = parameters_grid, cv=3, verbose=True, n_jobs=-1)

best_clf_lin = clf_lin.fit(X_train,y_train)

best_clf_lin.best_estimator_

print(f' Accuracy - : {best_clf_lin.score(X_train,y_train):.3f}')

